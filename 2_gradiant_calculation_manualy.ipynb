{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In PyTorch, requires_grad is an attribute of a Tensor that indicates whether or not the gradients with respect to this tensor should be computed during the backward pass. When you perform backpropagation, PyTorch calculates gradients for all tensors with requires_grad=True. These gradients are stored in the .grad attribute of the tensor.\n",
    "\n",
    "Why is requires_grad used?\n",
    "\n",
    "Training Models: In deep learning, you often adjust the weights of your model to minimize the loss function. To update the weights using optimization algorithms like gradient descent, you need to compute the gradient of the loss function with respect to each weight. By setting requires_grad=True on the model's weights, PyTorch will automatically compute these gradients for you during backpropagation.\n",
    "\n",
    "Freezing Layers: In transfer learning or fine-tuning, you might want to freeze certain layers of a pre-trained model (i.e., not update them during training). You can do this by setting requires_grad=False for the parameters of those layers.\n",
    "\n",
    "Memory Efficiency: If you don't need gradients for a specific tensor (e.g., input data or when performing inference), setting requires_grad=False saves memory and computation time.\n",
    "\n",
    "requires_grad is used to automatically compute the gradients needed for optimizing model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1049,  0.8008, -0.7523,  0.0189], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 .. Add gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1469,  1.7618, -0.7824,  0.0947])\n"
     ]
    }
   ],
   "source": [
    "# y = x * 2\n",
    "# print(y)\n",
    "\n",
    "# y = x * 2\n",
    "# z = y * y * 3\n",
    "# print(z)\n",
    "\n",
    "# z = x * x * 2\n",
    "# z = z.mean()\n",
    "# z = z.median()\n",
    "# print(z)\n",
    "\n",
    "\n",
    "# z = x * x * 2\n",
    "# z = z.mean()\n",
    "# z.backward()        ## dz/dx\n",
    "# print(x.grad)       ## grad will work if the  requires_grad=True ..... grad can be implemented to a scaller product only\n",
    "\n",
    "\n",
    "\n",
    "z = x * x * 2\n",
    "v = torch.tensor([0.1, 0.3, 0.01, 1.0], dtype=torch.float32)\n",
    "z.backward(v)        ## dz/dx\n",
    "print(x.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Creation: We create two tensors x and y with requires_grad=True. This tells PyTorch to keep track of operations on these tensors and to compute their gradients when we call .backward().\n",
    "\n",
    "Operation: We compute z = x * y + y. PyTorch builds a computation graph and records the operations.\n",
    "\n",
    "Backward Pass: We call z.backward(), which computes the gradients of z with respect to x and y.\n",
    "\n",
    "Gradients: After the backward pass, x.grad contains the gradient of z with respect to x, and y.grad contains the gradient of z with respect to y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 .... Remove gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6976, 0.0641, 0.3627, 0.2815], requires_grad=True)\n",
      "tensor([2.6976, 2.0641, 2.3627, 2.2815])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(4, requires_grad=True)\n",
    "print(y)\n",
    "\n",
    "# y.requires_grad_(False)\n",
    "# print(y)\n",
    "\n",
    "# z = y.detach()\n",
    "# print(z)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = y + 2\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model sum tensor(5.4000, grad_fn=<SumBackward0>)\n",
      "model gardient  tensor([1.8000, 1.8000, 1.8000])\n",
      "model sum tensor(5.4000, grad_fn=<SumBackward0>)\n",
      "model gardient  tensor([1.8000, 1.8000, 1.8000])\n",
      "model sum tensor(5.4000, grad_fn=<SumBackward0>)\n",
      "model gardient  tensor([1.8000, 1.8000, 1.8000])\n",
      "model sum tensor(5.4000, grad_fn=<SumBackward0>)\n",
      "model gardient  tensor([1.8000, 1.8000, 1.8000])\n",
      "model sum tensor(5.4000, grad_fn=<SumBackward0>)\n",
      "model gardient  tensor([1.8000, 1.8000, 1.8000])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(3, requires_grad=True)\n",
    "\n",
    "for epochs in range(5):\n",
    "    model_output = (weights*1.8).sum()\n",
    "    print(\"model sum\", model_output)\n",
    "\n",
    "    model_output.backward()\n",
    "\n",
    "    print(\"model gardient \", weights.grad)\n",
    "\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
